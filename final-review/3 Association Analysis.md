# Association Analysis

## 1.Association rule definition



### Market Basket Analysis

- principal applcation of association rules is market basket analysis
- it models supermarket data and discover associations between different products that bought together by customers.



### Data Model

- A collection of transactions as a dataset(market basket).
- Each transaction is a set of items
- For binary association rule mining, the quantity of each item is not considered. only consider if it is bought.



### Notation

- D = dataset
- I = set of all items
- N = total number of transactions in D
- T = a transaction in D
- a,b,c,A,B,C,... = items
- X, Y, Z, ... = set of items(commonly called **itemsets**)
- transaction is an itemset



 ### Definition

- support count of X called **sup(X)**, is the number of of transaction in D that contain X
- if itemset X contains K items, called X is a K-itemset



### Association rules

- X, Y are **non-empty, disjoint** itemsets
- *support* condition: $sup(X\cup Y) / N  \geq p_s$
- *confidence* condition: $sup(X\cup Y) / sup(X)  \geq p_s$

### Usefulness and Certainty

#### $p_s$ = support threshold

- support condition ensure the rule concern items that occur in a good portion of the transaction.
- rules are of sufficient interest

#### $p_c$ = confidence threshold

- conditional probability
- confidence condition ensure the rule are of high confidence



### Find the rules

1. find all **frequent** itemsets
   - frequent if $sup(S) \geq N * p_s$

2. **generate rules** from frequent itemsets:
   - For each frequent itemset S in (1):
     - split S into *two non-empty disjoint* sebsets X, Y
     - check if X -> Y satisfies the confidence condition
     - repeat for all possible partitionings of S

- Step 1 could be expensive
- check frequent itemset is very computationaly expensive useing a brute-force approach



- Step 2 is relatively straightforward
- frequent k-itemset can generate $2^k - 2$ possible rules
- This exhaustive approach is usually not a problem for generating rules when K is small for applicaitions.













## 2.The Apriori Algorithm



 ### The Apriori property:

- if X is a frequent itemset, any non-empty subset also frequent.
- if X is not frequent itemset, any super subset also not frequent.
- Apriori Algorithm use this property to prune the large number of itemsets.



### The Apriori Algorithm

1. 1st iteration:
   - scan dataset to find all frequent 1-itemset. i.e. $L_1$
2. 2nd iteration:
   - scan to find X is $C_2$ if it contains 2 items that all size 1-subset are frequent
   - scan to find frequent 2-itemset and put into $L_2$
3. Its iteration:
   - san to find X is $C_i$ if it contains i items that all size (i-1)-subset are frequent
   - scan to find  frequent i-itemset and put into $L_i$



- From $L_{i-1}$  to  $C_i$ :
  - **apriori-gen**
  - it consider all pairs of (i-1)-itemsets X and Y from $L_{i-1}$ that share $i-2$ common items.
  - Each such i-itemset Z generated will be checked if  all of its (i-1)-itemset are in $L_{i-1}$.
  - If Yes, put into $C_i$





## 3.Improving Efficiency



#### 1.Transaction reduction

- if a transaction does not contibute any support to any candidate itemset in $C_i$, it will not contibute in the later iteration. This transaction could be safely discarded after i-th iteration.

#### 2.Sampling

- Approach 1: Instead of scanning the whole dataset, only a fraction of the dataset is used
- Approach 2: "negative border" = $U_i(C_i-L_i)$
  - An itemset X is in the "negative border" if X is infrequent and all of its subsets are frequent.

#### 3.Hashing



#### 4. Distributed Algorithms

- Simple Algorithms

  - k conputers $S_k$

  - D paritition into k parts: $D_k$

  - **locally frequent**

  - **globally frequent**

- X is globlly frequent -> X is locally frequent at some $S_i$
- X is not locally frequent at any site -> X is not globally frequent



- A better Algotithm
  - X is globally frequent, then exists a site $S_i$ such that all X's subsets are locally frequent at $S_i$
  - candidate k-itemset X should be generated iff all (k-1) subsets are locally frequent at $S_i$
  - X should be generated by $S_i$ if all its (k-1) subsets are globally frequent and they are locally frequent at $S_i$



## 4.Compact representation of mining result

### Maximal Frequent Itemset

- maximal frequent is that if (1)it is frequent and (2) none of its immediate supersets is frequents

### Closed Itemset

- itemset X is closed if none of its immediate super sets has the same support as X.

### MFT VS CI

<img src="/Users/Pking/Library/Application Support/typora-user-images/image-20211114162105306.png" alt="image-20211114162105306" style="zoom:25%;" />



### High-utility itemset

- instead of its support, we could evaluate the importance of an itemset by its utility.

- Data model given:

  - $q(a,T)$

  - unit-profit $p(a)$
  - $\sigma(X,T) = \sum(p(a)*q(a,T))$
  - $\pi(X,D) = \sum\sigma(X,T)$

- The utility of itemset X in a dataset D:
  - $\pi(X,D)$

- for all itemset X, find $\pi(X,D) \geq min_u$, where the $min_u$ is a *minimum utility threshold*





## 5.The FP-growth algorithm



### FP-growth

- a alternative apporach to Apriorit algorithm
- compress a large db into a compact **Frequent-Pattern tree (FP-tree)** structure
  - Store the structure in memory to avoid costly db scans
- run an efficient, FP-tree-based frequent pattern mining method
  - A divide-and-conquer strategy
  - Avoid candidate generation and subset testing



### FP-tress Construction

- steps:
  1. scan DB once, find frequent 1-itemsets
  2. order frequent items in descending frequence order
  3. scan and construct FP



### Mining Frequent Patterns Using FP-tree

- Idea: recursively grow frequent path using fp-tree
- Method:
  - For each item, construct its *conditional pattern-base and conditional FP-tree*
  - Repeat the process on each newly created conditional Fp-tree
  - Until the Fp-tree is null or only one path.

References: [fp-tree data structure](https://www.cnblogs.com/pinard/p/6307064.html)



### Summary

- Advantages:
  - No candidate generation, no candidate test
  - use compact data structure
  - No repeated db scan, thus reduce I/O
  - Basic operation is counting and construct FP-tree
  - Usually more efficient than Apriori
- Disadvantages:
  - For very large DB, it may not fit in DB very well.



## 6.Interestingness

- #### Pattern Evaluation

  - Association rule algorithms tend to produce too many rules

  - Interestingness measures can be used to prune or rank the derived partterns

  - support and confidence are the only measures used in the original



- #### Drawback of Confidence

- #### Statistical Independence

  - $P(S\cap B) = P(S) * P(B)$ ---> Statistical independence

  - $P(S\cap B) > P(S) * P(B)$ ---> positively correlated

  - $P(S\cap B) < P(S) * P(B)$ ---> negatively correlated

- #### Lift

  - $Lift = \frac{P(Y|X)}{P(Y)} = \frac{P(X\cap Y)}{P(X)P(Y)}$

  

## 7.Mining quantitative association rules



- #### Quantitative Association Rules(QAR)

  - QAR deal with, not only binary attributes, but also quantitative attributes
  - Rule:
    - X --> Y: where X and Y are subsets of $I_R$ and that no attribute appears in both X and Y

- #### Support and Confidence

  - set $X\subseteq I_R$, record r in D support X if r contain values that fall into the corresponding value in X

- #### QAR Summary

  - Mining QAR is a lot more expensive than mining binary association cos the number of items considered in QAR is larger
  - each discrete numerical value with a domain of t values can get $O(t^2)$ Items

  

- #### Map QAR to binary model

  - The partitioning problem
    - The partitioning of a quantative attribute into intervals cannot be too fine or too coarse
      - Too fine means too specific to derive rules
      - Too coarse means too general to be useful
    - One method is to specify a maximum support(maxsup) parameter:
      - Intervals are allowed to merge with neighboring intervals as long as the resulting support does not exceed maxsup.
  - The fragmented rules problem
    - combined rule would never be generated in out example cos the item is not generated
    - Rules merging need to be done to handle the fragmented rule problem
    - Rule that share then same right side and having same set in left side is possible to merge

- #### Dense-Region-Based Approach

  - Each rectangular region defines a rule
  - More denser, more specific
  - regios is *dense* if density satisfy density threshold $p_d$
  - scenarios is ineffective?
    - High dimension, very sparse

  

## 8.mining sequential patterns

- #### sequence data

  - A sequence s is an ordered list of transaction and each transaction contians a collection of items

  - k-sequence is a sequence that contain k items.

    

- #### Sequence A contained in Sequence B

- #### Generalized Sequential Pattern(GSP)

  - Step1: make the first pass over D to yield all 1-item frequent sequences
  - Step2: (Repeat)
    - Candidate Generation: merge (k-1)st pass to generate that contain k items
    - Candidate Pruning: Prune candidate K-sequences that contain infrequent (k-1)-subsequences
    - Support Counting: make a new pass to find the support for new candiadate sequences
  - **Candidate Generation**:
    - Base rule(k = 2):
      - $i_1$ and $i_2$ will produce $<\{i_1\}\{i_2\}>,<\{i_2\}\{i_1\}>,<\{i_1 i_2\}>$ 
    - General rule(k > 2):
      - frequent (k-1)-sequence $w_1$ and $w_2$ to produce k-sequence: if $w_1$ Remove first is same as $w_2$ remove last item, then merge
      - $w_1$ extend the last item of $w_2$
        - if last two items in $w_2$ belong to same transaction
        - Otherwise

- #### Other application of frequent patterns

  - Emerging patternsËœ
  - Statistical debugging





